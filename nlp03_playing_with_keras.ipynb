{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing with Keras\n",
    "21 June 2017, 양철웅\n",
    "\n",
    "좀 살펴보니 개발 초기에 Tensorflow로 직접 하는 경우가 많지 않다. Tensorflow는 기계학습의 저수준 API이고, 상위 계층 Wrapper인 Keras를 사용하는 것 같다. (Tensorflow에도 후에 상위 API가 나왔는데, 그 전부터 사용되던 Keras 가 보급되어 있고, Keras는 Tensorflow contrib package로 들어가 있다.) 그래서 Tensorflow도 모르면서 바로 상위 수준의 Keras를 만져보기로 한다. AI/머신러닝쪽 학습 장벽이 만만치가 않다. 이론적 학습서를 제외하고서라도 다음 접근이 필요하다.\n",
    " - ML 기본 (Coursera 비디오 코스, Hands-on machine learning 책)\n",
    " - 딥러닝 기본\n",
    " - Python\n",
    " - Numpy, Scikit-learn\n",
    " - Tensorflow\n",
    " - Keras\n",
    " \n",
    "이것들 모두 한개도 모르지만 일단 최상위것을 건드려보자. \n",
    "\n",
    "## Keras 설치\n",
    "1. 이전 문서에서 설명한 virtualenv 활성화 된 상태에서 Keras를 설치한다.\n",
    "```\n",
    "(env) $ pip install --upgrade keras\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Keras 테스트\n",
    "\n",
    "신경망을 이용해서 $f(x,y) = x^2y + y + 2$ 일때 $f(3,4)$ 값을 계산해 보도록 하자.\n",
    "https://keras.io/getting-started/sequential-model-guide/ 의 튜토리얼을 적당히 고쳐보았다.\n",
    "히든레이어는 하나만 두어보자.\n",
    "\n",
    "아래 학습을 시키는데에도 많은 시행착오 및 인터넷 검색, 질문이 필요했다.\n",
    " - 신경망 학습시 Dropout이 필요하다고 해서 Dropout을 세팅했더니 학습이 되지 않았다. Dropout을 빼보라는 답변을 얻어 Dropout을 뺐더니 학습이 됐다\n",
    " - 초기에는 data 1000개로 32유닛 1계층, 10유닛 1계층으로 학습했는데 잘 안되어서, data 50000개, 100유닛으로 각각 2계층을 만들었더니 학습이 됐다.\n",
    " \n",
    "언제 dropout을 넣고, 히든 레이어 유닛수를 몇개를 잡아야 하는지에 대해서는 감이 없는 상태이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 2)\n",
      "(50000, 1)\n",
      "Epoch 1/100\n",
      "50000/50000 [==============================] - 1s - loss: 63890.0688     \n",
      "Epoch 2/100\n",
      "50000/50000 [==============================] - 0s - loss: 56782.2083     \n",
      "Epoch 3/100\n",
      "50000/50000 [==============================] - 0s - loss: 51385.9148     \n",
      "Epoch 4/100\n",
      "50000/50000 [==============================] - 0s - loss: 47316.0130     \n",
      "Epoch 5/100\n",
      "50000/50000 [==============================] - 0s - loss: 43142.2000     \n",
      "Epoch 6/100\n",
      "50000/50000 [==============================] - 0s - loss: 37088.3210     \n",
      "Epoch 7/100\n",
      "50000/50000 [==============================] - 0s - loss: 32621.7222     \n",
      "Epoch 8/100\n",
      "50000/50000 [==============================] - 0s - loss: 28947.5913     \n",
      "Epoch 9/100\n",
      "50000/50000 [==============================] - 0s - loss: 25753.2269     \n",
      "Epoch 10/100\n",
      "50000/50000 [==============================] - 0s - loss: 22856.4544     \n",
      "Epoch 11/100\n",
      "50000/50000 [==============================] - 0s - loss: 20273.9130     - ETA: 1s - loss: 22227.204 - ETA: 0s - l\n",
      "Epoch 12/100\n",
      "50000/50000 [==============================] - 1s - loss: 17970.1279     \n",
      "Epoch 13/100\n",
      "50000/50000 [==============================] - 0s - loss: 15947.8117     \n",
      "Epoch 14/100\n",
      "50000/50000 [==============================] - 0s - loss: 14147.5136     \n",
      "Epoch 15/100\n",
      "50000/50000 [==============================] - 0s - loss: 12538.7393     \n",
      "Epoch 16/100\n",
      "50000/50000 [==============================] - 0s - loss: 11098.1305     \n",
      "Epoch 17/100\n",
      "50000/50000 [==============================] - 0s - loss: 9805.9319     \n",
      "Epoch 18/100\n",
      "50000/50000 [==============================] - 0s - loss: 8650.0569     \n",
      "Epoch 19/100\n",
      "50000/50000 [==============================] - 0s - loss: 7612.8596     \n",
      "Epoch 20/100\n",
      "50000/50000 [==============================] - 0s - loss: 6683.4993     \n",
      "Epoch 21/100\n",
      "50000/50000 [==============================] - 0s - loss: 5851.4317     \n",
      "Epoch 22/100\n",
      "50000/50000 [==============================] - 0s - loss: 5082.3185     \n",
      "Epoch 23/100\n",
      "50000/50000 [==============================] - 1s - loss: 4412.1107     \n",
      "Epoch 24/100\n",
      "50000/50000 [==============================] - 0s - loss: 3826.2339     \n",
      "Epoch 25/100\n",
      "50000/50000 [==============================] - 0s - loss: 3311.5856     \n",
      "Epoch 26/100\n",
      "50000/50000 [==============================] - 0s - loss: 2859.3135     \n",
      "Epoch 27/100\n",
      "50000/50000 [==============================] - 0s - loss: 2463.6763     \n",
      "Epoch 28/100\n",
      "50000/50000 [==============================] - 0s - loss: 2118.0120     \n",
      "Epoch 29/100\n",
      "50000/50000 [==============================] - 0s - loss: 1816.3789     \n",
      "Epoch 30/100\n",
      "50000/50000 [==============================] - ETA: 0s - loss: 1543.31 - 1s - loss: 1553.7828     \n",
      "Epoch 31/100\n",
      "50000/50000 [==============================] - 1s - loss: 1324.5422     \n",
      "Epoch 32/100\n",
      "50000/50000 [==============================] - 1s - loss: 1126.3072     \n",
      "Epoch 33/100\n",
      "50000/50000 [==============================] - 1s - loss: 954.8423     \n",
      "Epoch 34/100\n",
      "50000/50000 [==============================] - 1s - loss: 802.5649     \n",
      "Epoch 35/100\n",
      "50000/50000 [==============================] - 0s - loss: 669.5189     \n",
      "Epoch 36/100\n",
      "50000/50000 [==============================] - 1s - loss: 558.5235     \n",
      "Epoch 37/100\n",
      "50000/50000 [==============================] - 1s - loss: 464.3359     \n",
      "Epoch 38/100\n",
      "50000/50000 [==============================] - 0s - loss: 384.6396     \n",
      "Epoch 39/100\n",
      "50000/50000 [==============================] - 0s - loss: 317.4874     \n",
      "Epoch 40/100\n",
      "50000/50000 [==============================] - 0s - loss: 261.4207     \n",
      "Epoch 41/100\n",
      "50000/50000 [==============================] - 0s - loss: 214.3027     \n",
      "Epoch 42/100\n",
      "50000/50000 [==============================] - 0s - loss: 175.0428     \n",
      "Epoch 43/100\n",
      "50000/50000 [==============================] - 1s - loss: 142.7175     \n",
      "Epoch 44/100\n",
      "50000/50000 [==============================] - 1s - loss: 115.7011     \n",
      "Epoch 45/100\n",
      "50000/50000 [==============================] - 0s - loss: 93.7843     \n",
      "Epoch 46/100\n",
      "50000/50000 [==============================] - 0s - loss: 75.7464     \n",
      "Epoch 47/100\n",
      "50000/50000 [==============================] - 0s - loss: 61.1364     \n",
      "Epoch 48/100\n",
      "50000/50000 [==============================] - 0s - loss: 49.0126     \n",
      "Epoch 49/100\n",
      "50000/50000 [==============================] - 0s - loss: 39.2966     \n",
      "Epoch 50/100\n",
      "50000/50000 [==============================] - 0s - loss: 31.4530     \n",
      "Epoch 51/100\n",
      "50000/50000 [==============================] - 0s - loss: 25.2219     \n",
      "Epoch 52/100\n",
      "50000/50000 [==============================] - 0s - loss: 19.9930     \n",
      "Epoch 53/100\n",
      "50000/50000 [==============================] - 0s - loss: 16.2259     \n",
      "Epoch 54/100\n",
      "50000/50000 [==============================] - 0s - loss: 12.8977     \n",
      "Epoch 55/100\n",
      "50000/50000 [==============================] - 0s - loss: 10.2766     \n",
      "Epoch 56/100\n",
      "50000/50000 [==============================] - 0s - loss: 8.1921     \n",
      "Epoch 57/100\n",
      "50000/50000 [==============================] - 0s - loss: 6.7315     \n",
      "Epoch 58/100\n",
      "50000/50000 [==============================] - 0s - loss: 5.2664     \n",
      "Epoch 59/100\n",
      "50000/50000 [==============================] - 0s - loss: 4.2093     \n",
      "Epoch 60/100\n",
      "50000/50000 [==============================] - 0s - loss: 3.3788     \n",
      "Epoch 61/100\n",
      "50000/50000 [==============================] - 0s - loss: 2.7761     \n",
      "Epoch 62/100\n",
      "50000/50000 [==============================] - 0s - loss: 2.3034     \n",
      "Epoch 63/100\n",
      "50000/50000 [==============================] - 0s - loss: 2.0192     \n",
      "Epoch 64/100\n",
      "50000/50000 [==============================] - 0s - loss: 1.6784     \n",
      "Epoch 65/100\n",
      "50000/50000 [==============================] - 0s - loss: 1.4377     \n",
      "Epoch 66/100\n",
      "50000/50000 [==============================] - 0s - loss: 1.1765     \n",
      "Epoch 67/100\n",
      "50000/50000 [==============================] - 0s - loss: 1.0471     \n",
      "Epoch 68/100\n",
      "50000/50000 [==============================] - 1s - loss: 0.9029     \n",
      "Epoch 69/100\n",
      "50000/50000 [==============================] - 0s - loss: 0.7790     \n",
      "Epoch 70/100\n",
      "50000/50000 [==============================] - 0s - loss: 0.6690     \n",
      "Epoch 71/100\n",
      "50000/50000 [==============================] - 0s - loss: 0.5713     \n",
      "Epoch 72/100\n",
      "50000/50000 [==============================] - 0s - loss: 0.6181     \n",
      "Epoch 73/100\n",
      "50000/50000 [==============================] - 0s - loss: 0.4726     \n",
      "Epoch 74/100\n",
      "50000/50000 [==============================] - 0s - loss: 0.4585     \n",
      "Epoch 75/100\n",
      "50000/50000 [==============================] - 0s - loss: 0.4983     \n",
      "Epoch 76/100\n",
      "50000/50000 [==============================] - 0s - loss: 0.3636     \n",
      "Epoch 77/100\n",
      "50000/50000 [==============================] - 0s - loss: 0.2918     \n",
      "Epoch 78/100\n",
      "50000/50000 [==============================] - 0s - loss: 0.3619     \n",
      "Epoch 79/100\n",
      "50000/50000 [==============================] - 0s - loss: 0.2936     \n",
      "Epoch 80/100\n",
      "50000/50000 [==============================] - 0s - loss: 0.3375     \n",
      "Epoch 81/100\n",
      "50000/50000 [==============================] - 0s - loss: 0.2427     \n",
      "Epoch 82/100\n",
      "50000/50000 [==============================] - 1s - loss: 0.2947     \n",
      "Epoch 83/100\n",
      "50000/50000 [==============================] - 0s - loss: 0.2768     \n",
      "Epoch 84/100\n",
      "50000/50000 [==============================] - 0s - loss: 0.2805     \n",
      "Epoch 85/100\n",
      "50000/50000 [==============================] - 0s - loss: 0.2563     \n",
      "Epoch 86/100\n",
      "50000/50000 [==============================] - 0s - loss: 0.2471     \n",
      "Epoch 87/100\n",
      "50000/50000 [==============================] - 0s - loss: 0.2115     \n",
      "Epoch 88/100\n",
      "50000/50000 [==============================] - 0s - loss: 0.2333     \n",
      "Epoch 89/100\n",
      "50000/50000 [==============================] - 0s - loss: 0.2672     \n",
      "Epoch 90/100\n",
      "50000/50000 [==============================] - 0s - loss: 0.2049     \n",
      "Epoch 91/100\n",
      "50000/50000 [==============================] - 1s - loss: 0.1335     \n",
      "Epoch 92/100\n",
      "50000/50000 [==============================] - 0s - loss: 0.2432     \n",
      "Epoch 93/100\n",
      "50000/50000 [==============================] - 0s - loss: 0.1647     \n",
      "Epoch 94/100\n",
      "50000/50000 [==============================] - 0s - loss: 0.3638     \n",
      "Epoch 95/100\n",
      "50000/50000 [==============================] - 0s - loss: 0.1010     \n",
      "Epoch 96/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 1s - loss: 0.1424     \n",
      "Epoch 97/100\n",
      "50000/50000 [==============================] - 1s - loss: 0.1782     \n",
      "Epoch 98/100\n",
      "50000/50000 [==============================] - 0s - loss: 0.1471     \n",
      "Epoch 99/100\n",
      "50000/50000 [==============================] - 0s - loss: 0.1744     \n",
      "Epoch 100/100\n",
      "50000/50000 [==============================] - 0s - loss: 0.1665     \n",
      "1/1 [==============================] - 0s\n",
      "3^2*4+4+2 = array([[ 41.95339584]], dtype=float32) (should be 42)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Generate data\n",
    "num_data=50000\n",
    "x_train = np.random.random((num_data,2))* 10\n",
    "x = x_train[:, 0]\n",
    "y = x_train[:, 1]\n",
    "y_train = x*x*y + y + 2\n",
    "y_train.shape=(num_data,1)\n",
    "print x_train.shape\n",
    "print y_train.shape\n",
    "x_test = np.array([3,4])\n",
    "x_test.shape=(1,2)\n",
    "model = Sequential()\n",
    "\n",
    "# 이렇게 하면 되나 몰라..\n",
    "# Dense(64) is a fully-connected layer with 64 hidden units.\n",
    "model.add(Dense(100, activation='sigmoid', input_dim=2))\n",
    "model.add(Dense(100, activation='sigmoid'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          epochs=100,\n",
    "          batch_size=128)\n",
    "y = model.predict(x_test, batch_size=32, verbose=1)\n",
    "print \"3^2*4+4+2 = %r (should be 42)\" % y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
